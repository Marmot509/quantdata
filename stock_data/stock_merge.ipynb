{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取所有股票代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the updated function to modify the filenames as required\n",
    "def get_ticker_with_prefix(path):\n",
    "    tickers = []\n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.csv'):\n",
    "            # Extract numeric part\n",
    "            numeric_part = ''.join(filter(str.isdigit, f))\n",
    "            # Extract the letter part (.SZ or .SH), convert to lowercase and prepend to the numeric part\n",
    "            letter_part = f.split('.')[1].lower()\n",
    "            #ticker = letter_part + numeric_part\n",
    "            ticker = numeric_part\n",
    "            tickers.append(ticker)\n",
    "    # Sort the list of IDs\n",
    "    return sorted(tickers)\n",
    "\n",
    "# Replace 'path_to_folder' with the actual path to your folder containing the CSV files\n",
    "path_to_folder = 'data/raw-data/2024/2024'\n",
    "\n",
    "# Get the modified stock ids\n",
    "tickers_with_prefix = get_ticker_with_prefix(path_to_folder)\n",
    "\n",
    "# Convert the list of modified stock ids to JSON format\n",
    "json_content = json.dumps(tickers_with_prefix, indent=4)\n",
    "\n",
    "# Replace 'path_to_json_file' with the actual path where you want to save the JSON file\n",
    "path_to_json_file = 'data/tickers.json'\n",
    "\n",
    "# Write the JSON content to a file\n",
    "with open(path_to_json_file, 'w') as json_file:\n",
    "    json_file.write(json_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按股票合并市值最高股票按年的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging Data: 100%|██████████| 696/696 [2:45:47<00:00, 14.29s/it]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# 路径设置\n",
    "json_file = 'data/top_cap_tickers.json'\n",
    "data_dir = 'data/raw-data/'\n",
    "merged_dir = 'data/by_stock_merged_top_caps/'\n",
    "if not os.path.exists(merged_dir):\n",
    "    os.makedirs(merged_dir)\n",
    "\n",
    "# 读取股票代码列表\n",
    "with open(json_file, 'r') as f:\n",
    "    tickers = json.load(f)\n",
    "\n",
    "# 为每个股票代码合并数据\n",
    "for ticker in tickers:\n",
    "    all_data = []  # 存储单个股票的所有数据\n",
    "    # 遍历每个年份的文件夹\n",
    "    for year in os.listdir(data_dir):\n",
    "        year_dir = os.path.join(data_dir, year)\n",
    "        if os.path.isdir(year_dir):  # 确保是目录\n",
    "            # 假设后缀可能是.SZ或.SH，尝试两种可能性\n",
    "            for suffix in ['.SZ', '.SH']:\n",
    "                file_path = os.path.join(year_dir, f\"{ticker}{suffix}.csv\")\n",
    "                if os.path.isfile(file_path):  # 确保文件存在\n",
    "                    # 读取CSV文件的指定列\n",
    "                    data = pd.read_csv(file_path)\n",
    "                    all_data.append(data)\n",
    "                    break  # 如果找到文件，则不需要尝试另一个后缀\n",
    "    \n",
    "    if all_data:\n",
    "        # 合并数据\n",
    "        merged_data = pd.concat(all_data)\n",
    "        # 按交易时间排序\n",
    "        merged_data.sort_values(by='trade_time', inplace=True)\n",
    "        # 保存到merged文件夹\n",
    "        merged_data.to_csv(os.path.join(merged_dir, f\"{ticker}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并所有股票的列数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging files: 100%|██████████| 692/692 [49:35<00:00,  4.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_csv_files(folder_path, output_folder, column):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    merged_data = pd.DataFrame()\n",
    "    files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    for file in tqdm(files, desc='Merging files with column ' + column):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path, parse_dates=['trade_time'])\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "        df.rename(columns={column: file_name}, inplace=True)\n",
    "        df = df[['trade_time', file_name]]\n",
    "        \n",
    "        if merged_data.empty:\n",
    "            merged_data = df\n",
    "        else:\n",
    "            merged_data = pd.merge_ordered(merged_data, df, on='trade_time', fill_method='ffill')\n",
    "\n",
    "    \n",
    "    output_file = os.path.join(output_folder, f'merged_{column}.csv')\n",
    "    merged_data.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "columns = [\n",
    "#    'open',\n",
    "#    'high', \n",
    "#    'low', \n",
    "    'close', \n",
    "#    'vol', \n",
    "    'amount'\n",
    "    ]\n",
    "folder_path = 'data/by_stock_merged_top_caps/'  \n",
    "output_folder = 'data/merged_top_caps/'  \n",
    "\n",
    "for column in columns:\n",
    "    merge_csv_files(folder_path, output_folder, column)\n",
    "    print(f'Merged {column} successfully!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据检查"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切片前五列数据用于快速发现问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'data/merged_top_caps/merged_close.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# extract the first 5 columns\n",
    "df = df.iloc[:, :5]\n",
    "df.to_csv('data/merged_top_caps/merged_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查数据文件总行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1398000, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# folder_path = 'data/merged_top_caps/'\n",
    "\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith('.csv'):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         print(filename, df.shape)\n",
    "#         print('\\n')\n",
    "\n",
    "file_path = 'data/merged_top_caps/merged_test.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查是否每天有240个数据点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "2000-01-04    True\n",
      "2000-01-05    True\n",
      "2000-01-06    True\n",
      "2000-01-07    True\n",
      "2000-01-10    True\n",
      "              ... \n",
      "2024-01-08    True\n",
      "2024-01-09    True\n",
      "2024-01-10    True\n",
      "2024-01-11    True\n",
      "2024-01-12    True\n",
      "Length: 5825, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'data/merged_top_caps/merged_test.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 检查是否每天有240个数据点\n",
    "df['trade_time'] = pd.to_datetime(df['trade_time'])\n",
    "df['date'] = df['trade_time'].dt.date\n",
    "grouped = df.groupby('date').size()\n",
    "result = grouped == 240\n",
    "print(result)\n",
    "\n",
    "invalid_dates = grouped[grouped != 240].index.tolist()\n",
    "for i in invalid_dates:\n",
    "    print(i, grouped[i])\n",
    "\n",
    "# # list all the data points in trade_time column in the invalid dates\n",
    "# invalid_data = df[df['date'].isin(invalid_dates)]\n",
    "# print(invalid_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理某些日期多一个数据点的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_close.csv: 已完成处理\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_csv_files(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            temp_file_path = os.path.join(folder_path, f\"temp_{filename}\")\n",
    "\n",
    "            try:\n",
    "                # 确定文件的总行数\n",
    "                total_rows = sum(1 for row in open(file_path, 'r', encoding='utf-8'))\n",
    "\n",
    "                # 读取除了最后10万行的所有数据\n",
    "                read_rows = total_rows - 100000  \n",
    "                data = pd.read_csv(file_path, nrows=read_rows)\n",
    "                data.to_csv(temp_file_path, index=False)\n",
    "\n",
    "                # 处理最后10万行\n",
    "                data_last_part = pd.read_csv(file_path, skiprows=read_rows + 1, header=None)\n",
    "                data_last_part.columns = data.columns  # 确保列名一致\n",
    "                data_last_part = data_last_part[data_last_part['trade_time'] != '2023-11-30 13:00:00']\n",
    "\n",
    "                # 如果最后10万行的trade_time数据有重复，删除重复的trade_time数据所在行\n",
    "                data_last_part.drop_duplicates(subset=['trade_time'], inplace=True)\n",
    "\n",
    "                # 将处理后的最后一部分追加到临时文件\n",
    "                data_last_part.to_csv(temp_file_path, mode='a', index=False, header=False)\n",
    "\n",
    "                # 替换原始文件\n",
    "                os.remove(file_path)\n",
    "                os.rename(temp_file_path, file_path)\n",
    "\n",
    "                print(f\"{filename}: 已完成处理\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"处理文件 {filename} 时出错: {e}\")\n",
    "\n",
    "process_csv_files('data/merged_top_caps/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将分钟收盘价处理为收益率并后复权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tickers: 100%|██████████| 4/4 [00:00<00:00, 104.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed period 1m.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tickers: 100%|██████████| 4/4 [00:00<00:00, 82.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed period 5m.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tickers: 100%|██████████| 4/4 [00:00<00:00, 110.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed period 15m.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tickers: 100%|██████████| 4/4 [00:00<00:00, 114.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed period 60m.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_data(file_path):\n",
    "    df = pd.read_csv(file_path, index_col='trade_time')\n",
    "    df.index = pd.to_datetime(df.index)  # 确保trade_time是DatetimeIndex类型\n",
    "    return df\n",
    "\n",
    "def process_period_data(df, period, minutes):\n",
    "    if period == '1d':\n",
    "        df_period = df.groupby(df.index.date).last()\n",
    "    else:\n",
    "        minutes_since_930 = (df.index.hour * 60 + df.index.minute) - (9 * 60 + 30)\n",
    "        is_not_break_time = ~((df.index.hour == 11) & (df.index.minute > 30)) & ~((df.index.hour == 13) & (df.index.minute < 1))\n",
    "        df_period = df[(minutes_since_930 % minutes == 0) & is_not_break_time]\n",
    "    \n",
    "    return df_period\n",
    "\n",
    "def process_hfq_data(df_period_rate, hfq_data_path, ticker, minutes):\n",
    "    file_found = False\n",
    "    for prefix in ['sh', 'sz']:\n",
    "        filename = f'{prefix}{ticker}.csv'\n",
    "        file_path = os.path.join(hfq_data_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            hfq_data = pd.read_csv(file_path)\n",
    "            hfq_data['date'] = pd.to_datetime(hfq_data['date'])\n",
    "            file_found = True\n",
    "            break\n",
    "\n",
    "    if file_found:\n",
    "        for index, row in hfq_data[hfq_data['date'] > '2000-01-01'].iterrows():\n",
    "            additional_hours, additional_minutes = divmod(30 + minutes, 60)\n",
    "            trade_time = row['date'] + datetime.timedelta(hours=9 + additional_hours, minutes=additional_minutes)\n",
    "            if trade_time in df_period_rate.index:\n",
    "                df_period_rate.at[trade_time, ticker] = (df_period_rate.at[trade_time, ticker] + 1) * row['hfq_one_point'] - 1\n",
    "    else:\n",
    "        print(f'No hfq_one_point data file found for ticker {ticker}. Skipping...')\n",
    "\n",
    "    return df_period_rate\n",
    "\n",
    "# 主逻辑\n",
    "file_path = 'data/merged_top_caps/merged_test.csv'\n",
    "hfq_data_path = 'data/backward_adjust_factor'\n",
    "df = read_data(file_path)\n",
    "\n",
    "data_period = {\n",
    "    '1d': 0,  # 特殊处理，0表示日数据\n",
    "    '1m': 1,\n",
    "    '5m': 5, \n",
    "    '15m': 15, \n",
    "    '60m': 60,\n",
    "}\n",
    "\n",
    "for period, minutes in data_period.items():\n",
    "    df_period = process_period_data(df, period, minutes)\n",
    "    df_period_rate = df_period.pct_change()\n",
    "    df_period_rate.fillna(0, inplace=True)\n",
    "\n",
    "    for ticker in tqdm(df_period_rate.columns, desc=\"Processing tickers\"):\n",
    "        df_period_rate = process_hfq_data(df_period_rate, hfq_data_path, ticker, minutes)\n",
    "\n",
    "    df_period_rate.to_csv(f'data/period_rate/{period}.csv', index=True)\n",
    "    df_period_rate.to_pickle(f'data/period_rate/{period}.pkl')\n",
    "    print(f'Processed period {period}.')\n",
    "\n",
    "\n",
    "# file_path = 'data/merged_top_caps/merged_test.csv'\n",
    "# df = pd.read_csv(file_path, index_col='trade_time')\n",
    "# df.index = pd.to_datetime(df.index)  # 确保trade_time是DatetimeIndex类型\n",
    "# hfq_data_path = 'data/backward_adjust_factor'\n",
    "\n",
    "# data_period = {\n",
    "#     '1m': 1,\n",
    "#     # '2m': 2, \n",
    "#     # '3m': 3,\n",
    "#     '5m': 5, \n",
    "#     '15m': 15, \n",
    "#     # '30m': 30, \n",
    "#     '60m': 60,\n",
    "# }\n",
    "\n",
    "# for period, minutes in data_period.items():\n",
    "#     if minutes == 1:\n",
    "#         df_period = df.copy()\n",
    "#     else:\n",
    "#         minutes_since_930 = (df.index.hour * 60 + df.index.minute) - (9 * 60 + 30)\n",
    "#         # 排除午休时间（11:30 - 13:00）\n",
    "#         is_not_break_time = ~((df.index.hour == 11) & (df.index.minute > 30)) & ~((df.index.hour == 13) & (df.index.minute < 1))\n",
    "#         df_period = df[(minutes_since_930 % minutes == 0) & is_not_break_time]\n",
    "\n",
    "#     df_period_rate = df_period.pct_change()\n",
    "#     df_period_rate.fillna(0, inplace=True)\n",
    "\n",
    "#     for ticker in tqdm(df_period_rate.columns, desc=\"Processing tickers\"):\n",
    "#         file_found = False\n",
    "#         for prefix in ['sh', 'sz']:\n",
    "#             filename = f'{prefix}{ticker}.csv'\n",
    "#             file_path = os.path.join(hfq_data_path, filename)\n",
    "#             if os.path.exists(file_path):\n",
    "#                 hfq_data = pd.read_csv(file_path)\n",
    "#                 hfq_data['date'] = pd.to_datetime(hfq_data['date'])\n",
    "#                 file_found = True\n",
    "#                 break\n",
    "\n",
    "#         if file_found:\n",
    "#             for index, row in hfq_data[hfq_data['date'] > '2000-01-01'].iterrows():\n",
    "#                 additional_hours, additional_minutes = divmod(30 + minutes, 60)\n",
    "#                 trade_time = row['date'] + datetime.timedelta(hours=9 + additional_hours, minutes=additional_minutes)\n",
    "#                 if trade_time in df_period_rate.index:\n",
    "#                     df_period_rate.at[trade_time, ticker] = (df_period_rate.at[trade_time, ticker] + 1) * row['hfq_one_point'] - 1\n",
    "\n",
    "#         else:\n",
    "#             print(f'No hfq_one_point data file found for ticker {ticker}. Skipping...')\n",
    "\n",
    "#     df_period_rate.to_csv(f'data/period_rate/{period}.csv', index=True)\n",
    "#     df_period_rate.to_pickle(f'data/period_rate/{period}.pkl')\n",
    "#     print(f'Processed period {period}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 收益率数据检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 15m.csv 的每天数据点数不同，请检查以下日期的数据：\n",
      "日期：2023-11-30, 数据点数：15\n",
      "文件 60m.csv 的每天数据点数不同，请检查以下日期的数据：\n",
      "日期：2023-11-30, 数据点数：3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 5m.csv 的每天数据点数不同，请检查以下日期的数据：\n",
      "日期：2023-11-30, 数据点数：47\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = 'data/period_rate/'\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, file), index_col=0)\n",
    "    df.index = pd.to_datetime(df.index).date\n",
    "    \n",
    "    counts = df.groupby(df.index).size()\n",
    "    \n",
    "    if len(set(counts)) != 1:\n",
    "        print(f'文件 {file} 的每天数据点数不同，请检查以下日期的数据：')\n",
    "        for date, count in counts[counts != counts.mode()[0]].items():\n",
    "            print(f'日期：{date}, 数据点数：{count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
