{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取所有股票代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the updated function to modify the filenames as required\n",
    "def get_ticker_with_prefix(path):\n",
    "    tickers = []\n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith('.csv'):\n",
    "            # Extract numeric part\n",
    "            numeric_part = ''.join(filter(str.isdigit, f))\n",
    "            # Extract the letter part (.SZ or .SH), convert to lowercase and prepend to the numeric part\n",
    "            letter_part = f.split('.')[1].lower()\n",
    "            #ticker = letter_part + numeric_part\n",
    "            ticker = numeric_part\n",
    "            tickers.append(ticker)\n",
    "    # Sort the list of IDs\n",
    "    return sorted(tickers)\n",
    "\n",
    "# Replace 'path_to_folder' with the actual path to your folder containing the CSV files\n",
    "path_to_folder = 'data/raw-data/2024'\n",
    "\n",
    "# Get the modified stock ids\n",
    "tickers_with_prefix = get_ticker_with_prefix(path_to_folder)\n",
    "\n",
    "# Convert the list of modified stock ids to JSON format\n",
    "json_content = json.dumps(tickers_with_prefix, indent=4)\n",
    "\n",
    "# Replace 'path_to_json_file' with the actual path where you want to save the JSON file\n",
    "path_to_json_file = 'data/tickers.json'\n",
    "\n",
    "# Write the JSON content to a file\n",
    "with open(path_to_json_file, 'w') as json_file:\n",
    "    json_file.write(json_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取后复权因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 从 akshare 获取后复权因子数据，并保存在 CSV 文件中\n",
    "\n",
    "import akshare as ak\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 读取股票代码\n",
    "path_to_json_file = 'data/ticker.json'\n",
    "with open(path_to_json_file, 'r') as json_file:\n",
    "    tickers = json.load(json_file)\n",
    "\n",
    "# 检查并获取后复权因子\n",
    "for ticker in tickers:\n",
    "    filename = f\"{ticker}.csv\"\n",
    "\n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(\"data/backward_adjust_factor/\" + filename):\n",
    "        # 如果ticker开头是0或3，增加sz前缀，否则增加sh前缀\n",
    "        if ticker.startswith('0') or ticker.startswith('3'):\n",
    "            ticker = 'sz' + ticker\n",
    "        else:\n",
    "            ticker = 'sh' + ticker\n",
    "            \n",
    "        try:\n",
    "            # 尝试调用 API 获取数据\n",
    "            df = ak.stock_zh_a_daily(symbol=ticker, adjust=\"hfq-factor\")\n",
    "            # 保存数据到 CSV 文件\n",
    "            df.to_csv(\"data/backward_adjust_factor/\" + filename, index=False)\n",
    "            print(f\"{ticker} data saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            # 打印错误信息，并继续处理下一个 ticker\n",
    "            print(f\"Error retrieving data for {ticker}: {e}\")\n",
    "    else:\n",
    "        print(f\"File {filename} already exists. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按股票合并历年数据<多线程>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging Stocks:  32%|███▏      | 1623/5100 [1:56:44<19:40:49, 20.38s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 路径设置\n",
    "json_file = 'data/tickers.json'\n",
    "data_dir = 'data/raw-data/'\n",
    "merged_dir = 'data/by_stock_merged/'\n",
    "if not os.path.exists(merged_dir):\n",
    "    os.makedirs(merged_dir)\n",
    "\n",
    "# 读取股票代码列表\n",
    "with open(json_file, 'r') as f:\n",
    "    tickers = json.load(f)\n",
    "\n",
    "def merge_data_for_ticker(ticker):\n",
    "    # 检查合并后的文件是否已存在\n",
    "    output_file = os.path.join(merged_dir, f\"{ticker}.csv\")\n",
    "    if os.path.isfile(output_file):\n",
    "        return  # 如果已存在，则跳过这个股票\n",
    "    \n",
    "    all_data = []  # 存储单个股票的所有数据\n",
    "    # 遍历每个年份的文件夹\n",
    "    for year in os.listdir(data_dir):\n",
    "        year_dir = os.path.join(data_dir, year)\n",
    "        if os.path.isdir(year_dir):  # 确保是目录\n",
    "            # 假设后缀可能是.SZ或.SH，尝试两种可能性\n",
    "            for suffix in ['.SZ', '.SH']:\n",
    "                file_path = os.path.join(year_dir, f\"{ticker}{suffix}.csv\")\n",
    "                if os.path.isfile(file_path):  # 确保文件存在\n",
    "                    data = pd.read_csv(file_path)\n",
    "                    all_data.append(data)\n",
    "                    break\n",
    "    \n",
    "    if all_data:\n",
    "        # 合并数据\n",
    "        merged_data = pd.concat(all_data)\n",
    "        # 按交易时间排序\n",
    "        merged_data.sort_values(by='trade_time', inplace=True)\n",
    "        # 保存到merged文件夹\n",
    "        merged_data.to_csv(os.path.join(merged_dir, f\"{ticker}.csv\"), index=False)\n",
    "\n",
    "def main():\n",
    "    max_workers = max(1, os.cpu_count() - 1)  # 保留一个CPU核心\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # 设置tqdm进度条\n",
    "        futures = [executor.submit(merge_data_for_ticker, ticker) for ticker in tickers]\n",
    "        for _ in tqdm(concurrent.futures.as_completed(futures), total=len(tickers), desc=\"Merging Stocks\"):\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单只股票数据检查<多线程>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker: 000638, Invalid Dates: []\n",
      "Non-numeric data found in Ticker: 000638\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "def check_data_points_and_nan(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['trade_time'] = pd.to_datetime(df['trade_time'])\n",
    "    df['date'] = df['trade_time'].dt.date\n",
    "    grouped = df.groupby('date').size()\n",
    "\n",
    "    ticker = Path(file_path).stem  # 获取文件名作为股票代码\n",
    "    invalid_dates = grouped[grouped != 240].index.tolist()\n",
    "    nan_values = df.isna().any(axis=1)\n",
    "\n",
    "    if invalid_dates or nan_values.any():\n",
    "        return ticker, invalid_dates, df[nan_values]\n",
    "\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    folder_path = 'data/by_stock_merged/'  # 更改为您的数据文件夹路径\n",
    "    file_paths = Path(folder_path).glob('*.csv')\n",
    "\n",
    "    max_workers = max(1, os.cpu_count() - 1)  # 保留一个CPU核心\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = executor.map(check_data_points_and_nan, file_paths)\n",
    "        \n",
    "        for result in results:\n",
    "            if result:\n",
    "                ticker, invalid_dates, nan_data = result\n",
    "                print(f'Ticker: {ticker}, Invalid Dates: {invalid_dates}')\n",
    "                if not nan_data.empty:\n",
    "                    print(f'Non-numeric data found in Ticker: {ticker}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异常数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "files = [\n",
    "    '000638.csv'\n",
    "]\n",
    "file_folder = 'data/by_stock_merged/'\n",
    "return_folder = 'data/by_stock_return_rate/'\n",
    "\n",
    "for file in files:\n",
    "    file_path = file_folder + file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df = df[df.loc[:, 'open':'amount'].isna().all(axis=1) & df['trade_time'].notna()]    # 去掉空值\n",
    "    df = df[df['trade_time'] != '2023-11-30 13:00:00']  # 去掉2023-11-30 13:00:00这一行\n",
    "    df = df.drop_duplicates(subset='trade_time')    # 去掉重复行\n",
    "\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    # 如果文件存在，则删除by_stock_return_rate中的对应文件\n",
    "    return_file_path = return_folder + file\n",
    "    if os.path.exists(return_file_path):\n",
    "        os.remove(return_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取历年流动性<多线程>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import akshare as ak\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_stock_data(year, ticker):\n",
    "    # Prepare start and end dates\n",
    "    start_date = f\"{year}0101\"\n",
    "    end_date = f\"{year}1231\"\n",
    "\n",
    "    try:\n",
    "        # Fetch data from akshare\n",
    "        turnover = ak.stock_zh_a_hist(symbol=ticker, start_date=start_date, end_date=end_date, adjust='')[['换手率']]\n",
    "        amount = ak.stock_zh_a_hist(symbol=ticker, start_date=start_date, end_date=end_date, adjust='')[['成交额']]\n",
    "\n",
    "        if turnover.empty or amount.empty:\n",
    "            return None\n",
    "\n",
    "        trading_days = len(turnover)\n",
    "        daily_average_turnover = turnover['换手率'].mean() / 100\n",
    "        circulating_market_cap = amount['成交额'].iloc[-1] / turnover['换手率'].iloc[-1] * 100\n",
    "\n",
    "        return [ticker, daily_average_turnover, circulating_market_cap, trading_days]\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        with open('data/tickers.json') as f:\n",
    "            tickers = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading tickers file: {e}\")\n",
    "        return\n",
    "\n",
    "    years = range(2000, 2024)\n",
    "    max_workers = max(1, os.cpu_count() - 1)  # 保留一个CPU核心\n",
    "\n",
    "    turnover_dir = 'data/turnover'\n",
    "    if not os.path.exists(turnover_dir):\n",
    "        os.makedirs(turnover_dir)\n",
    "\n",
    "    for year in years:\n",
    "        results = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # 在这里为每个ticker创建一个进度条\n",
    "            futures = {executor.submit(process_stock_data, year, ticker): ticker for ticker in tickers}\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(tickers), desc=f\"Processing {year}\"):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results.append(result)\n",
    "\n",
    "        if results:\n",
    "            df = pd.DataFrame(results, columns=['tickers', 'turnover_ratio', 'circulating_market_cap', 'trading_days'])\n",
    "            df.set_index('tickers', inplace=True)\n",
    "            df.sort_values(by='turnover_ratio', ascending=False, inplace=True)\n",
    "            filename = os.path.join(turnover_dir, f'turnovers_{year}.csv')\n",
    "            df.to_csv(filename)\n",
    "            df.to_pickle(os.path.join(turnover_dir, f'turnovers_{year}.pkl'))\n",
    "            print(f\"Data for {year} saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按股票计算收益率并进行后复权处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hfq_one_point data file found for ticker 002396. Skipping...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['return_rate'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m period \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1m\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhfq_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(input_folder, output_folder, hfq_data_path, period, period_dict)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# 如果输出文件已存在，跳过该股票\u001b[39;00m\n\u001b[1;32m     53\u001b[0m stock_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m---> 54\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_hfq_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhfq_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m processed_data\u001b[38;5;241m.\u001b[39mto_csv(output_file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 37\u001b[0m, in \u001b[0;36mprocess_hfq_data\u001b[0;34m(stock_data, hfq_data_path, ticker, period, period_dict)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo hfq_one_point data file found for ticker \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstock_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrade_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreturn_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['return_rate'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "# 定义后复权处理函数\n",
    "def process_hfq_data(stock_data, hfq_data_path, ticker, period, period_dict):\n",
    "    file_found = False\n",
    "    for prefix in ['sh', 'sz']:\n",
    "        filename = f'{prefix}{ticker}.csv'\n",
    "        file_path = os.path.join(hfq_data_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            hfq_data = pd.read_csv(file_path)\n",
    "            hfq_data['date'] = pd.to_datetime(hfq_data['date'])\n",
    "            file_found = True\n",
    "            break\n",
    "\n",
    "    if file_found:\n",
    "        stock_data['trade_time'] = pd.to_datetime(stock_data['trade_time'])\n",
    "        stock_data['return_rate'] = stock_data['close'].pct_change().fillna(0)\n",
    "\n",
    "        for index, row in hfq_data[hfq_data['date'] > '2000-01-01'].iterrows():\n",
    "            if period == '1d':\n",
    "                trade_time = row['date'].date()\n",
    "            else:\n",
    "                minutes = period_dict[period]\n",
    "                additional_hours, additional_minutes = divmod(30 + minutes, 60)\n",
    "                trade_time = row['date'] + datetime.timedelta(hours=9 + additional_hours, minutes=additional_minutes)\n",
    "\n",
    "            mask = (stock_data['trade_time'].dt.date == trade_time) if period == '1d' else (stock_data['trade_time'] == trade_time)\n",
    "\n",
    "            if mask.any():\n",
    "                stock_data.loc[mask, 'return_rate'] = (stock_data.loc[mask, 'return_rate'] + 1) * row['hfq_one_point'] - 1\n",
    "\n",
    "        return stock_data[['trade_time', 'return_rate']]  # 仅保留交易时间和收益率\n",
    "\n",
    "    else:\n",
    "        print(f'No hfq_one_point data file found for ticker {ticker}. Skipping...')\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(input_folder, output_folder, hfq_data_path, period, period_dict):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for file_path in glob.glob(os.path.join(input_folder, '*.csv')):\n",
    "        ticker = os.path.basename(file_path).split('.')[0]\n",
    "        output_file_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "\n",
    "        # 检查输出文件是否已存在\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"Output for {ticker} already exists. Skipping...\")\n",
    "            continue  # 如果输出文件已存在，跳过该股票\n",
    "\n",
    "        stock_data = pd.read_csv(file_path)\n",
    "        processed_data = process_hfq_data(stock_data, hfq_data_path, ticker, period, period_dict)\n",
    "        if processed_data is not None:\n",
    "            processed_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# 定义不同周期的分钟数\n",
    "period_dict = {\n",
    "    '1m': 1,\n",
    "    '5m': 5,\n",
    "    # ... 其他周期\n",
    "}\n",
    "\n",
    "input_folder = 'data/by_stock_merged'\n",
    "output_folder = 'data/by_stock_return_rate'\n",
    "hfq_data_path = 'data/backward_adjust_factor'\n",
    "period = '1m'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(input_folder, output_folder, hfq_data_path, period, period_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
